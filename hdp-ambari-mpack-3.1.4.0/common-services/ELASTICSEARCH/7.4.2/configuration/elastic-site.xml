<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  "License"); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!-- Elastic search  Configurations -->

<configuration supports_final="true">
    <!-- Configurations -->
    <property>
        <name>cluster_name</name>
        <value>elasticsearch</value>
        <description>Elasticsearch Cluster Name identifies your Elasticsearch subsystem</description>
    </property>
    <property>
        <name>masters_also_are_datanodes</name>
        <value>"true"</value>
        <description>ES Masters and Slaves cannot be installed on the same nodes.  Set this to "true" if you want the ES master nodes to serve as combined master/datanodes. Note: surround value in quotes.</description>
        <value-attributes>
            <type>string</type>
        </value-attributes>
    </property>
    <property>
        <name>discovery_seed_hosts</name>
        <value></value>
        <description>This should be the fqdn hostname(s) with quotes: ["host1"]</description>
    </property>
    <property>
        <name>cluster_initial_master_nodes</name>
        <value>[]</value>
        <description>Bootstrap the cluster using an initial set of master-eligible nodes.  This should be the same as Node Name (hostname). Use Quotes: ["node.hostname.com"]</description>
    </property>
    <property>
        <name>index_number_of_shards</name>
        <value>4</value>
        <description>Set the number of shards (splits) of an index.  Changes are not effective after index creation. Usually set to 1 for single-node install.</description>
    </property>
    <property>
        <name>index_number_of_replicas</name>
        <value>2</value>
        <description>Set the number of replicas (copies in addition to the first) of an index. Usually set to 0 for single-node install.</description>
    </property>
    <property>
        <name>path_data</name>
        <value>"/opt/lmm/es_data"</value>
        <description>Comma-separated list of directories where to store index data allocated for each node: "/mnt/first","/mnt/second".  Number of paths should relate to number of shards, and preferably should be on separate physical volumes.</description>
    </property>
    <property>
        <name>http_cors_enabled</name>
        <value>"true"</value>
        <description>Enable or disable cross-origin resource sharing, i.e. whether a browser on another origin can do requests to Elasticsearch. Defaults to true.</description>
        <value-attributes>
            <type>string</type>
        </value-attributes>
    </property>
    <property>
        <name>http_port</name>
        <value>9200</value>
        <description>Set a custom port to listen for HTTP traffic</description>
    </property>
    <property>
        <name>transport_tcp_port</name>
        <value>9300</value>
        <description>Set a custom port for the node to node communication</description>
    </property>
    <!--  Multi-node Discovery -->
    <property>
        <name>discovery_zen_ping_timeout</name>
        <value>3s</value>
        <description>Wait for ping responses for master discovery</description>
    </property>
    <property>
        <name>discovery_zen_fd_ping_interval</name>
        <value>15s</value>
        <description>Wait for ping for cluster discovery</description>
    </property>
    <property>
        <name>discovery_zen_fd_ping_timeout</name>
        <value>60s</value>
        <description>Wait for ping for cluster discovery</description>
    </property>
    <property>
        <name>discovery_zen_fd_ping_retries</name>
        <value>5</value>
        <description>Number of ping retries before blacklisting</description>
    </property>
    <!--  Gateway -->
    <property>
        <name>gateway_recover_after_data_nodes</name>
        <value>1</value>
        <description>Recover as long as this many data or master nodes have joined the cluster.</description>
    </property>
    <property>
        <name>recover_after_time</name>
        <value>15m</value>
        <description>recover_after_time</description>
    </property>
    <property>
        <name>expected_data_nodes</name>
        <value>0</value>
        <description>expected_data_nodes</description>
    </property>
    <!--  Index -->  
    <property>
        <name>index_merge_scheduler_max_thread_count</name>
        <value>5</value>
        <description>index.merge.scheduler.max_thread_count</description>
    </property>
    <property>
        <name>indices_memory_index_store_throttle_type</name>
        <value>none</value>
        <description>index_store_throttle_type</description>
    </property>
    <property>
        <name>index_refresh_interval</name>
        <value>1s</value>
        <description>index refresh interval</description>
    </property>
    <property>
        <name>index_translog_flush_threshold_size</name>
        <value>5g</value>
        <description>index_translog_flush_threshold_size</description>
    </property>
    <property>
        <name>indices_memory_index_buffer_size</name>
        <value>10%</value>
        <description>Percentage of heap used for write buffers</description>
    </property>
    <property>
        <name>bootstrap_memory_lock</name>
        <value>true</value>
        <description>The third option on Linux/Unix systems only, is to use mlockall to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out</description>
    </property>
    <property>
        <name>threadpool_bulk_queue_size</name>
        <value>3000</value>
        <description>It tells ES the number of  requests that can be queued for execution in the node when there is no thread available to execute a bulk request</description>
    </property>
    <property>
        <name>threadpool_index_queue_size</name>
        <value>1000</value>
        <description>It tells ES the number of  requests that can be queued for execution in the node when there is no thread available to execute index request</description>
    </property>
    <property>
        <name>indices_cluster_send_refresh_mapping</name>
        <value>false</value>
        <description>In order to make the index request more efficient, we have set this property on our data nodes</description>
    </property>
    <property>
        <name>indices_fielddata_cache_size</name>
        <value>25%</value>
        <description>You need to keep in mind that not setting this value properly can cause:Facet searches and sorting to have very poor performance:The ES node to run out of memory if you run the facet query against a large index</description>
    </property>
    <property>
        <name>cluster_routing_allocation_disk_watermark_high</name>
        <value>0.99</value>
        <description>Property used when multiple drives are used to understand max thresholds</description>
    </property>
    <property>
        <name>cluster_routing_allocation_disk_threshold_enabled</name>
        <value>true</value>
        <description>Property used when multiple drives are used to understand if thresholding is active</description>
    </property>   
   <property>
        <name>cluster_routing_allocation_disk_watermark_low</name>
        <value>.97</value>
        <description>Property used when multiple drives are used to understand min thresholds</description>
    </property>
    <property>
        <name>cluster_routing_allocation_node_concurrent_recoveries</name>
        <value>4</value>
        <description>Max concurrent recoveries, useful for fast recovery of the cluster nodes on restart</description>
    </property>
</configuration>
